import asyncio
import aiohttp
import aiofiles
import os
import json
import re
import logging
import gzip
from parsel import Selector
from urllib.parse import urljoin
from rule_manager import RuleManager  # ç¡®ä¿ rule_manager.py åœ¨åŒçº§ç›®å½•

# ==========================================
# ğŸ”§ é…ç½®åŒºåŸŸ (Configuration Area)
# ==========================================

# === æ—¥å¿—é…ç½® (æ¯” print æ›´ä¸“ä¸š) ===
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# === å…¨å±€é…ç½® ===
CONCURRENCY = 5  # å¹¶å‘æ•°: åŒæ—¶ä¸‹è½½5ç«  (å»ºè®®ä¸è¦è¶…è¿‡10ï¼Œä»¥å…è¢«å°)
DELAY = 0.5      # æ¯æ¬¡è¯·æ±‚åçš„ç¤¼è²Œå»¶è¿Ÿ (ç§’)
RETRIES = 3      # å¤±è´¥é‡è¯•æ¬¡æ•°

# === è¯·æ±‚å¤´ ==
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

# ==========================================
# ğŸ› ï¸ æ ¸å¿ƒé€»è¾‘ (Core Logic)
# ==========================================



def clean_filename(filename):
    """æ¸…æ´—æ–‡ä»¶åï¼Œç§»é™¤ Windows/Linux ä¸å…è®¸çš„å­—ç¬¦"""
    return re.sub(r'[\\/*?:"<>|]', "", filename).strip()

async def fetch(session, url, encoding='auto'):
    """é€šç”¨è¯·æ±‚å‡½æ•° (å¸¦åŸºç¡€é‡è¯•)"""
    for i in range(RETRIES):
        try:
            async with session.get(url, headers=headers, timeout=15) as response:
                if response.status == 200:
                    # è¯»å–åŸå§‹å­—èŠ‚
                    raw = await response.read()
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ gzip å‹ç¼© (é­”æ•°: 0x1f 0x8b)
                    if raw[:2] == b'\x1f\x8b':
                        try:
                            raw = gzip.decompress(raw)
                        except Exception as e:
                            print(f"âš ï¸ gzip è§£å‹å¤±è´¥: {e}")
                    
                    # è§£ç ä¸ºå­—ç¬¦ä¸²
                    if encoding != 'auto':
                        # ä½¿ç”¨æŒ‡å®šç¼–ç 
                        try:
                            return raw.decode(encoding)
                        except (UnicodeDecodeError, LookupError):
                            print(f"âš ï¸ ä½¿ç”¨æŒ‡å®šç¼–ç  {encoding} å¤±è´¥ï¼Œå°è¯•è‡ªåŠ¨æ£€æµ‹...")
                    
                    # è‡ªåŠ¨å°è¯•å¤šç§ç¼–ç 
                    for enc in ('utf-8', 'gb18030', 'gbk', 'gb2312', 'big5'):
                        try:
                            text = raw.decode(enc)
                            return text
                        except (UnicodeDecodeError, LookupError):
                            continue
                    
                    # éƒ½å¤±è´¥äº†ï¼Œç”¨ utf-8 å¿½ç•¥é”™è¯¯
                    return raw.decode('utf-8', errors='ignore')
                else:
                    print(f"âš ï¸ è¯·æ±‚å¤±è´¥ [{response.status}]: {url}")
                    return None
        except Exception as e:
            print(f"âŒ è¿æ¥å¼‚å¸¸ (ç¬¬{i+1}æ¬¡): {url} - {e}")
        await asyncio.sleep(1) # å¤±è´¥åç¨å¾®ç­‰ä¸€ä¸‹å†é‡è¯•
    return None

async def save_chapter(novel_dir, chapter_idx, title, content):
    """ä¿å­˜ç« èŠ‚åˆ°æ–‡ä»¶"""
    # æ–‡ä»¶åæ ¼å¼: 0001_ç¬¬ä¸€ç« .txt (åŠ å…¥åºå·æ–¹ä¾¿æ’åº)
    filename = f"{chapter_idx:04d}_{clean_filename(title)}.txt"
    filepath = os.path.join(novel_dir, filename)

    try:
        async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:
            await f.write(title + "\n\n")
            await f.write(content)
    except Exception as e:
        logger.error(f"æ–‡ä»¶å†™å…¥å¤±è´¥: {filename} - {e}")

async def download_chapter(session, url, idx, rules, encoding, semaphore, novel_dir):
    """ä¸‹è½½å•ä¸ªç« èŠ‚çš„å·¥ä½œå•å…ƒ"""
    async with semaphore:  # é™åˆ¶å¹¶å‘
        print(f"â³ [{idx}] æ­£åœ¨ä¸‹è½½: {url} ...")
        html = await fetch(session, url, encoding)
        if not html:
            print(f"âš ï¸ [{idx}] æ­£æ–‡è·å–å¤±è´¥: {url} ...")
            return
        
        sel = Selector(text=html)
        
        # è§£ææ ‡é¢˜å’Œå†…å®¹
        title = sel.css(rules["chapter_title"]).get()
        # è·å–æ‰€æœ‰æ®µè½å¹¶æ‹¼æ¥
        content_lines = sel.css(rules["chapter_content"]).getall()
        # æ¸…æ´—æ•°æ®: å»é™¤é¦–å°¾ç©ºæ ¼ï¼Œç”¨æ¢è¡Œç¬¦è¿æ¥
        content = "\n".join([line.strip() for line in content_lines if line.strip()])
        
        if title and content:
            await save_chapter(novel_dir, idx, title, content)
            print(f"âœ… [{idx}] ä¿å­˜æˆåŠŸ: {title}")
        else:
            print(f"âš ï¸ [{idx}] è§£æå¤±è´¥ (å¯èƒ½æ˜¯è§„åˆ™é”™è¯¯æˆ–åçˆ¬): {url}")
        
        await asyncio.sleep(DELAY) # ç¤¼è²Œæ€§å»¶è¿Ÿ

async def main():
    # 1. è¾“å…¥ç›®æ ‡
    # è¿™é‡Œä»¥åå¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°ä¼ å…¥ï¼Œç°åœ¨å…ˆå†™åœ¨è¿™
    TARGET_URL = "https://www.mnwx.cc/book/419057/" # æ›¿æ¢ä½ çš„ç›®æ ‡
    NOVEL_NAME = "æˆ‘çš„ä¸€ä½ä»™å­é“å‹"

    # 2. åŠ è½½è§„åˆ™ (å…³é”®å˜åŒ–ç‚¹!)
    manager = RuleManager()
    site_config = manager.get_rule_by_url(TARGET_URL)

    if not site_config:
        print("ç¨‹åºç»ˆæ­¢ï¼šæ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„ç½‘ç«™è§„åˆ™ï¼Œè¯·å…ˆåœ¨ sites.yaml ä¸­é…ç½®ã€‚")
        return
    else:
        # ä»é…ç½®ä¸­æå–å…·ä½“è§„åˆ™
        rules = site_config['rules']
        encoding = site_config.get('encoding', 'utf-8')

    """ä¸»è°ƒåº¦å™¨"""
    print(f"ğŸš€ å¯åŠ¨çˆ¬è™«ï¼Œç›®æ ‡: {NOVEL_NAME}")
    
    # 1. åˆ›å»ºå­˜å‚¨ç›®å½•
    base_dir = "downloads"
    novel_dir = os.path.join(base_dir, NOVEL_NAME)
    os.makedirs(novel_dir, exist_ok=True)
    
    # 2. åˆå§‹åŒ–å¹¶å‘é™åˆ¶å™¨
    semaphore = asyncio.Semaphore(CONCURRENCY)
    
    async with aiohttp.ClientSession() as session:
        # 3. è·å–ç›®å½•é¡µ
        print("æ­£åœ¨è·å–ç›®å½•åˆ—è¡¨...")
        toc_html = await fetch(session, TARGET_URL)
        if not toc_html:
            print("âŒ æ— æ³•è®¿é—®ç›®å½•é¡µï¼Œç¨‹åºç»ˆæ­¢ã€‚")
            return

        # 4. è§£æç›®å½•
        sel = Selector(text=toc_html)
        links = sel.css(rules["chapter_list"])
        
        tasks = []
        print(f"ğŸ“– å‘ç° {len(links)} ä¸ªç« èŠ‚ï¼Œå‡†å¤‡å¼€å§‹ä¸‹è½½...")

        # ç”Ÿæˆå…ƒæ•°æ® (Simple Meta Data)
        meta_info = {
            "name": NOVEL_NAME,
            "url": TARGET_URL,
            "total_chapters": len(links),
            "status": "downloading"
        }
        with open(os.path.join(novel_dir, "meta.json"), "w", encoding="utf-8") as f:
            json.dump(meta_info, f, ensure_ascii=False, indent=2)

        # 5. åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
        for idx, link in enumerate(links):
            # æå–é“¾æ¥
            href = link.attrib.get(rules["chapter_link_attr"])
            if not href: continue
            
            # è¡¥å…¨ URL
            full_url = urljoin(TARGET_URL, href)
            
            # åˆ›å»ºä»»åŠ¡ (æ³¨æ„ï¼šidx+1 æ˜¯ä¸ºäº†è®©ç« èŠ‚åºå·ä»1å¼€å§‹)
            task = asyncio.create_task(
                download_chapter(session, full_url, idx+1, rules, encoding, semaphore, novel_dir)
            )
            tasks.append(task)
        
        # 6. æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        if tasks:
            await asyncio.gather(*tasks)
        else:
            print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•ç« èŠ‚é“¾æ¥ï¼Œè¯·æ£€æŸ¥ 'chapter_list' è§„åˆ™ï¼")

    print(f"ğŸ‰ å…¨éƒ¨ä»»åŠ¡å®Œæˆï¼æ–‡ä»¶ä¿å­˜åœ¨: {novel_dir}")

if __name__ == "__main__":
    # Windows ä¸‹ Python 3.8+ éœ€è¦è®¾ç½®äº‹ä»¶å¾ªç¯ç­–ç•¥ (é˜²æ­¢æŠ¥é”™)
    if os.name == 'nt':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())